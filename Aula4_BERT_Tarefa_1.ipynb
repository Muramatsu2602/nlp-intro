{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula4 BERT - Tarefa 1",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMv3oWtdLhlju+crlyFdqq2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib_45Kw92CwR",
        "outputId": "f8fe4103-3702-40b3-e9eb-d75e3a489b13"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 12.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 26.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=26654e28922842a8abca6acb2cbea78d31910ae361a4f77295059a595a377cbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr3r1xqn2lyd"
      },
      "source": [
        "**Função  de Cálculo de Score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoLDQ0n-MNEp"
      },
      "source": [
        "def calc_score_tarefa1(text, target_word, tokenizer, model, debug=False):\n",
        "\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "  #get indexs\n",
        "  target_index = tokenizer.convert_tokens_to_ids([target_word])[0]\n",
        "  masked_index = tokenized_text.index('[MASK]') \n",
        "\n",
        "  # Create the segments tensors.\n",
        "  segments_ids = [0] * len(tokenized_text)\n",
        "\n",
        "  # Convert inputs to PyTorch tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "  \n",
        "  # Predict all tokens\n",
        "  with torch.no_grad():\n",
        "      predictions = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "  # get words again (just to compare the confidence)\n",
        "  expected_token = tokenizer.convert_ids_to_tokens([target_index])[0]\n",
        "\n",
        "  # normalise between 0 and 1\n",
        "  predictions_candidates = torch.sigmoid(predictions[0][0][masked_index])\n",
        "  \n",
        "  predicted_index = torch.argmax(predictions_candidates).item()\n",
        "\n",
        "  predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "\n",
        "  # if word dont exist return 0 \n",
        "  if predicted_token == '[UNK]':\n",
        "    return 0\n",
        "\n",
        "  predictions_candidates = predictions_candidates.cpu().numpy()\n",
        "  target_bert_confiance = predictions_candidates[target_index]\n",
        "  predicted_bert_confience = predictions_candidates[predicted_index]\n",
        "\n",
        "\n",
        "  # \n",
        "  score = 1 - (target_bert_confiance-predicted_bert_confience if  target_bert_confiance > predicted_bert_confience else predicted_bert_confience-target_bert_confiance)\n",
        "  if debug:\n",
        "    print(\"predicted token ---> \", predicted_token, predicted_bert_confience)\n",
        "    print(\"expected token  ---> \",expected_token , target_bert_confiance)\n",
        "    print(\"Score:\", score)\n",
        "\n",
        "  return score"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLE5MbhbwXS-",
        "outputId": "0fa6e9bb-7423-4352-a9b5-170ee76ed3b9"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "#import logging\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = BertForMaskedLM.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model.eval()\n",
        "\n",
        "\n",
        "text = '[CLS] Especificamente, a capacidade impressionante desses insetos de se espremerem por qualquer espaço e aguentarem [MASK] de até 900 vezes seu próprio peso sem sofrer grandes danos [SEP]'\n",
        "#text = '[CLS] Chegamos na metade da temporada! Você disputou [MASK] contra todas as equipes do grupo uma vez e a agora inicia-se o returno [SEP]'\n",
        "\n",
        "target_word = 'pressões'\n",
        "\n",
        "# calculate score\n",
        "score = calc_score_tarefa1(text, target_word, tokenizer, model, debug=True)\n",
        "# print(\"Score: \", score)a1(text, target_word, tokenizer, model, debug=True)\n",
        "print(\"Score:\",score)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "predicted token --->  cargas 0.9999845\n",
            "expected token  --->  pressões 0.999969\n",
            "Score: 0.9999845027923584\n",
            "Score: 0.9999845027923584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW5Itb_Nyc5K"
      },
      "source": [
        "**BERT - Buscando o Contexto : Tarefa 1**\n",
        "- Metodo calc_score_tarefa1"
      ]
    }
  ]
}